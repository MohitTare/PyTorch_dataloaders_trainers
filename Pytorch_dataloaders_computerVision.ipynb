{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as tfms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch has a very useful feature called Datasets and Dataloaders. Dataloaders help us in efficient preparation of data by giving us predefined templates. These templates (also called base/parent classes) can help us utilise many of the existing features of these and also use our custom functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loading in PyTorch can be separated in 2 parts:\n",
    "\n",
    "1. Data must be wrapped on a Dataset parent class where the methods _ _getitem_ _ and _ _len_ _ must be overrided. Not that at this point the data is not loaded on memory. PyTorch will only load what is needed to the memory.\n",
    "2. Use a Dataloader that will actually read the data and put into memory.\n",
    "\n",
    "In below notebook we will see how we can create some common dataloaders for major computer vision problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Classification is one of the major problems we solve using Deep learning/Computer Vision. For Image Classification as in any kind of any classification problem the X consists of the image(images) and Y consists of the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageClassification(Dataset):\n",
    "    def __init__(self, labels, root_dir, subset=False, transform=None):\n",
    "        self.labels = labels\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.labels.iloc[idx, 0] # file name\n",
    "        fullname = os.path.join(self.root_dir, (img_name + '.jpeg'))\n",
    "        image = Image.open(fullname).convert('RGB')\n",
    "        labels = self.labels.iloc[idx, 1] # category_id\n",
    "        if self.transform:\n",
    "            #print('Transforming')\n",
    "            image = self.transform(image)\n",
    "        return image, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the label mapping file\n",
    "df_label = pd.read_csv('DiabeticRetinopathy/Train/trainLabels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the files\n",
    "filelist = pd.Series(data = [os.path.splitext(x)[0] for x in os.listdir('DiabeticRetinopathy/Train')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label = df_label[df_label.image.isin(filelist.values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Above operation can also be merged within init of the dataset by passing the csv file as an input to the constructor (\\__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomImageClassification(df_label,'DiabeticRetinopathy/Train/',transform=tfms.transforms.Compose(\n",
    "                [\n",
    "                    tfms.transforms.Resize((600,600)),\n",
    "                    tfms.transforms.ToTensor()\n",
    "                ]\n",
    "                ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset,batch_size=2,shuffle=True,num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_test,lb = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 600, 600])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2592, 3888, 3])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  0])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Localization / Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Image localization or Object detection based problems we have to predict a bounding box also in addition to the class of the image. \n",
    "We can have multiple classes of the image and hence multiple bounding boxes.\n",
    "\n",
    "Most common format of specifying the annotations of bounding boxes is by using xml files (aka in PASCAL VOC style)\n",
    "You can read more about PASCAL VOC format and dataset here - http://host.robots.ox.ac.uk/pascal/VOC/\n",
    "The Dataloader in this case should output the following \n",
    "\n",
    "1. X - images batch (N,C,H,W) N- batch size , C - channel , H & W - Height and Width respectively\n",
    "2. Y - label (N,) for each image we will have label\n",
    "3. Bbox - Bounding box tensor. This tensor usually has 2 extreme corners of bounding box (N,xmin,ymin,xmax,ymax). In certain cases we have the center coordinate along with width and hieght present (N,xcenter,ycenter,h,w)\n",
    "\n",
    "We can use the tensor in any format we want for further processing in the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Organization/Directory Structure**\n",
    "\n",
    "- We have 2 major folders in any object detection dataset. One folder will contain Images\n",
    "- Another folder will contain the annotation xml file for these images\n",
    "- We should have the same name for image and its respective annotation file (eg: racoon-1.jpeg , racoon-1.xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomObjectDetection(Dataset):\n",
    "    def __init__(self,img_dir,annot_dir,class_list,transform=None):\n",
    "        self.root_dir = img_dir\n",
    "        self.annot_dir = annot_dir\n",
    "        self.class_list = class_list\n",
    "        self.transform = transform\n",
    "        self.img_annot_map = self.form_img_annot_map\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.img_annot_map.shape[0]\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        img_name = self.img_annot_map.loc[idx, 'filename'] # file name\n",
    "        fullname = os.path.join(self.root_dir, img_name)\n",
    "        image = Image.open(fullname).convert('RGB')\n",
    "        labels = torch.IntTensor([self.img_annot_map.loc[idx, 'class']]) # category_id\n",
    "        bbox = torch.tensor(self.img_annot_map.loc[idx, ['xmin','ymin','xmax','ymax']].astype('float'))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return [image,bbox],labels\n",
    "    \n",
    "    @property\n",
    "    def form_img_annot_map(self):\n",
    "        xml_list = []\n",
    "        for xml_file in glob.glob(self.annot_dir + '/*.xml'):\n",
    "            tree = ET.parse(xml_file)\n",
    "            root = tree.getroot()\n",
    "            for member in root.findall('object'):\n",
    "                value = (root.find('filename').text,\n",
    "                         int(root.find('size')[0].text),\n",
    "                         int(root.find('size')[1].text),\n",
    "                         member[0].text,\n",
    "                         int(member[4][0].text),\n",
    "                         int(member[4][1].text),\n",
    "                         int(member[4][2].text),\n",
    "                         int(member[4][3].text)\n",
    "                     )\n",
    "                xml_list.append(value)\n",
    "        column_name = ['filename','width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
    "        xml_df = pd.DataFrame(xml_list, columns=column_name)\n",
    "        xml_df = xml_df[xml_df['class'].isin(self.class_list)]\n",
    "        xml_df['class'] = xml_df['class'].astype(\"category\").cat.codes\n",
    "        return xml_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = CustomObjectDetection('raccoon_dataset-master/images/','raccoon_dataset-master/annotations/',['raccoon'],\n",
    "                            transform=tfms.transforms.Compose(\n",
    "                            [\n",
    "                            tfms.transforms.Resize((600,600)),\n",
    "                            tfms.transforms.ToTensor()\n",
    "                            ]\n",
    "                        )\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test,batch_size=2,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_bbox,label = next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  81.,   88.,  522.,  408.],\n",
       "        [ 130.,    2.,  446.,  488.]])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_bbox[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the output of data loader contains 3 parts 1. Image Tensor 2. Tensor of Bounding boxes 3. Lables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
